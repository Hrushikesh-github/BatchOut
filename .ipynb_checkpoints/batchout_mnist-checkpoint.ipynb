{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train models with batchout layers on MNIST. The architecture is as mentioned in the paper. \n",
    "There are seven models in total, with the 1st model having no batchout and the others having batchout at different locations. The model architectures can be found in \"architectures.py\" module.\n",
    "The results of batchout models on test set are consistent with the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages, use cuda, initialize the data loaders\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from math import exp\n",
    "from batchout import BatchOut\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "mnist_train = datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size = 128, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 128, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the models\n",
    "from architectures import *\n",
    "\n",
    "# A function that performs standard training/evaluation\n",
    "def epoch(loader, model, opt=None):\n",
    "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
    "    total_loss, total_err = 0.,0.\n",
    "    for X,y in loader:\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        if not opt:\n",
    "            model.eval()\n",
    "            yp = model(X)\n",
    "            loss = nn.CrossEntropyLoss()(yp,y)\n",
    "        if opt:\n",
    "            model.train()\n",
    "            yp = model(X)\n",
    "            loss = nn.CrossEntropyLoss()(yp, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "0.112667\t0.055200\n",
      "0.067433\t0.032300\n",
      "0.063117\t0.030500\n",
      "0.060267\t0.030500\n",
      "0.061683\t0.031800\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchOut()\n",
      "  (2): ReLU()\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchOut()\n",
      "  (6): ReLU()\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Flatten()\n",
      "  (9): Dropout(p=0.5, inplace=False)\n",
      "  (10): Linear(in_features=800, out_features=256, bias=True)\n",
      "  (11): BatchOut()\n",
      "  (12): ReLU()\n",
      "  (13): Dropout(p=0.5, inplace=False)\n",
      "  (14): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Time taken is: 6.276788592338562\n",
      "Training the model\n",
      "0.115750\t0.053300\n",
      "0.066017\t0.033700\n",
      "0.060017\t0.030500\n",
      "0.060400\t0.031100\n",
      "0.058333\t0.030700\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchOut()\n",
      "  (2): ReLU()\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Flatten()\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Linear(in_features=800, out_features=256, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.5, inplace=False)\n",
      "  (12): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Time taken is: 6.040803714593252\n",
      "Training the model\n",
      "0.664217\t0.207500\n",
      "0.087983\t0.043600\n",
      "0.075433\t0.039800\n",
      "0.072083\t0.039900\n",
      "0.071867\t0.039700\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchOut()\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Flatten()\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Linear(in_features=800, out_features=256, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.5, inplace=False)\n",
      "  (12): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Time taken is: 5.18717988729477\n",
      "Training the model\n",
      "0.684367\t0.204100\n",
      "0.089400\t0.045200\n",
      "0.076683\t0.042200\n",
      "0.075033\t0.042400\n",
      "0.074217\t0.041900\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Flatten()\n",
      "  (7): Dropout(p=0.5, inplace=False)\n",
      "  (8): Linear(in_features=800, out_features=256, bias=True)\n",
      "  (9): BatchOut()\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.5, inplace=False)\n",
      "  (12): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Time taken is: 5.081249964237213\n",
      "Training the model\n",
      "0.729950\t0.260200\n",
      "0.096033\t0.048200\n",
      "0.083450\t0.045000\n",
      "0.082583\t0.044000\n",
      "0.080400\t0.043900\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchOut()\n",
      "  (2): ReLU()\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchOut()\n",
      "  (6): ReLU()\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Flatten()\n",
      "  (9): Dropout(p=0.5, inplace=False)\n",
      "  (10): Linear(in_features=800, out_features=256, bias=True)\n",
      "  (11): ReLU()\n",
      "  (12): Dropout(p=0.5, inplace=False)\n",
      "  (13): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Time taken is: 6.1254278739293415\n",
      "Training the model\n",
      "0.697667\t0.220900\n",
      "0.099817\t0.053200\n",
      "0.086017\t0.047500\n",
      "0.083817\t0.045600\n",
      "0.084533\t0.047600\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchOut()\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Flatten()\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Linear(in_features=800, out_features=256, bias=True)\n",
      "  (10): BatchOut()\n",
      "  (11): ReLU()\n",
      "  (12): Dropout(p=0.5, inplace=False)\n",
      "  (13): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Time taken is: 5.2910712917645775\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now we train the models with batchout present in them. THe accuracies obtained are:\n",
    "1. With no batchout removed: 94 % \n",
    "2. Batchout after Conv1: 95 %\n",
    "3. Batchout after Conv2: 93 %\n",
    "4. Batchout after both Conv1, Conv2 : 92\n",
    "5. Batchout after f1: 93 %\n",
    "6. Batchout after Conv2 and f1: 92 %\n",
    "\n",
    "The results are consistent with the paper\n",
    "'''\n",
    "models = [batchout_model_cnn_all, batchout_model_cnn_c1, batchout_model_cnn_c2, batchout_model_cnn_f1, batchout_model_cnn_c12, batchout_model_cnn_c2f1]\n",
    "for (i, model) in enumerate(models):\n",
    "    model.to(device)\n",
    "    opt = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "    print('Training the model')\n",
    "    start = time.time()\n",
    "    for t in range(15):\n",
    "        train_err, train_loss = epoch(train_loader, model, opt)\n",
    "        test_err, test_loss = epoch(test_loader, model)\n",
    "        if t == 4:\n",
    "            for param_group in opt.param_groups:\n",
    "                param_group[\"lr\"] = 1e-3\n",
    "        if t == 10:\n",
    "            for param_group in opt.param_groups:\n",
    "                param_group[\"lr\"] = 1e-5\n",
    "        if t % 4 == 0 or t==14:\n",
    "            print(*(\"{:.6f}\".format(i) for i in (train_err, test_err)), sep=\"\\t\")\n",
    "    \n",
    "    end = time.time()\n",
    "    torch.save(model.state_dict(), str(i) + '.pt')\n",
    "    print(model)\n",
    "    print(\"Time taken is: {}\".format((end - start) / 60 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Consider the original architecture. Train the model and evaluate. \n",
    "We obtain 93.5 % accuracy on test set. Further accuracy can be obtained by decreasing lr... etc \n",
    "'''\n",
    "model_cnn.to(device)\n",
    "\n",
    "opt = optim.SGD(model_cnn.parameters(), lr=1e-2)\n",
    "\n",
    "print('Training the model')\n",
    "\n",
    "start = time.time()\n",
    "for t in range(15):\n",
    "    train_err, train_loss = epoch(train_loader, model_cnn, opt)\n",
    "    test_err, test_loss = epoch(test_loader, model_cnn)\n",
    "    if t == 4:\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-3\n",
    "    if t == 10:\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-5\n",
    "    if t % 4 == 0 or t==14:\n",
    "        print(*(\"{:.6f}\".format(i) for i in (train_err, test_err)), sep=\"\\t\")\n",
    "end = time.time()\n",
    "torch.save(model_cnn.state_dict(), \"mnist_cnn.pt\")\n",
    "print(\"Time taken is: {}\".format((end - start)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
